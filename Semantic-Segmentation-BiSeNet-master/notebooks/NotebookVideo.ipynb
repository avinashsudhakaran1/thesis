{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kiko-PC\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 88 from C header, got 96 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "from keras.backend import resize_images\n",
    "from keras.layers import *\n",
    "print(tf.__version__)\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kiko-PC\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.applications.xception import preprocess_input\n",
    "\n",
    "\n",
    "model = load_model('checkpoints/model-016.h5',  custom_objects={'preprocess_input': preprocess_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    image = cv2.cvtColor(cv2.imread(path,-1), cv2.COLOR_BGR2RGB)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_label_info(csv_path):\n",
    "    \"\"\"\n",
    "    Retrieve the class names and label values for the selected dataset.\n",
    "    Must be in CSV format!\n",
    "\n",
    "    # Arguments\n",
    "        csv_path: The file path of the class dictionairy\n",
    "        \n",
    "    # Returns\n",
    "        Two lists: one for the class names and the other for the label values\n",
    "    \"\"\"\n",
    "    filename, file_extension = os.path.splitext(csv_path)\n",
    "    if not file_extension == \".csv\":\n",
    "        return ValueError(\"File is not a CSV!\")\n",
    "\n",
    "    class_names = []\n",
    "    label_values = []\n",
    "    with open(csv_path, 'r') as csvfile:\n",
    "        file_reader = csv.reader(csvfile, delimiter=',')\n",
    "        header = next(file_reader)\n",
    "        for row in file_reader:\n",
    "            class_names.append(row[0])\n",
    "            label_values.append([int(row[1]), int(row[2]), int(row[3])])\n",
    "        # print(class_dict)\n",
    "    return class_names, label_values\n",
    "\n",
    "def one_hot_it(label, label_values):\n",
    "    \"\"\"\n",
    "    Convert a segmentation image label array to one-hot format\n",
    "    by replacing each pixel value with a vector of length num_classes\n",
    "\n",
    "    # Arguments\n",
    "        label: The 2D array segmentation image label\n",
    "        label_values\n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of num_classes\n",
    "    \"\"\"\n",
    "    # st = time.time()\n",
    "    # w = label.shape[0]\n",
    "    # h = label.shape[1]\n",
    "    # num_classes = len(class_dict)\n",
    "    # x = np.zeros([w,h,num_classes])\n",
    "    # unique_labels = sortedlist((class_dict.values()))\n",
    "    # for i in range(0, w):\n",
    "    #     for j in range(0, h):\n",
    "    #         index = unique_labels.index(list(label[i][j][:]))\n",
    "    #         x[i,j,index]=1\n",
    "    # print(\"Time 1 = \", time.time() - st)\n",
    "\n",
    "    # st = time.time()\n",
    "    # https://stackoverflow.com/questions/46903885/map-rgb-semantic-maps-to-one-hot-encodings-and-vice-versa-in-tensorflow\n",
    "    # https://stackoverflow.com/questions/14859458/how-to-check-if-all-values-in-the-columns-of-a-numpy-matrix-are-the-same\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        # colour_map = np.full((label.shape[0], label.shape[1], label.shape[2]), colour, dtype=int)\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis = -1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "    # print(\"Time 2 = \", time.time() - st)\n",
    "\n",
    "    return semantic_map\n",
    "    \n",
    "def reverse_one_hot(image):\n",
    "    \"\"\"\n",
    "    Transform a 2D array in one-hot format (depth is num_classes),\n",
    "    to a 2D array with only 1 channel, where each pixel value is\n",
    "    the classified class key.\n",
    "\n",
    "    # Arguments\n",
    "        image: The one-hot format image \n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of 1, where each pixel value is the classified \n",
    "        class key.\n",
    "    \"\"\"\n",
    "    # w = image.shape[0]\n",
    "    # h = image.shape[1]\n",
    "    # x = np.zeros([w,h,1])\n",
    "\n",
    "    # for i in range(0, w):\n",
    "    #     for j in range(0, h):\n",
    "    #         index, value = max(enumerate(image[i, j, :]), key=operator.itemgetter(1))\n",
    "    #         x[i, j] = index\n",
    "\n",
    "    x = np.argmax(image, axis = -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    \"\"\"\n",
    "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
    "\n",
    "    # Arguments\n",
    "        image: single channel array where each value represents the class key.\n",
    "        label_values\n",
    "        \n",
    "    # Returns\n",
    "        Colour coded image for segmentation visualization\n",
    "    \"\"\"\n",
    "\n",
    "    # w = image.shape[0]\n",
    "    # h = image.shape[1]\n",
    "    # x = np.zeros([w,h,3])\n",
    "    # colour_codes = label_values\n",
    "    # for i in range(0, w):\n",
    "    #     for j in range(0, h):\n",
    "    #         x[i, j, :] = colour_codes[int(image[i, j])]\n",
    "    \n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "\n",
    "    return x\n",
    "\n",
    "import csv\n",
    "class_names_list, label_values =get_label_info(\"data/class_dict.csv\")\n",
    "class_names_string = \"\"\n",
    "for class_name in class_names_list:\n",
    "    if not class_name == class_names_list[-1]:\n",
    "        class_names_string = class_names_string + class_name + \", \"\n",
    "    else:\n",
    "        class_names_string = class_names_string + class_name\n",
    "\n",
    "num_classes = len(label_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_final(img):\n",
    "    img = img[200:-500,:]\n",
    "\n",
    "    img= cv2.resize(img, dsize=(608, 608))\n",
    "\n",
    "    img = np.array([img])\n",
    "    t = model.predict([img, img])\n",
    "    output_image = reverse_one_hot(t)\n",
    "    out_vis_image = colour_code_segmentation(output_image, label_values)\n",
    "    a = cv2.cvtColor(np.uint8(out_vis_image[0]),1)\n",
    "    b=cv2.cvtColor(np.uint8(img[0]),1)\n",
    "    added_image = cv2.addWeighted(a,1,b,1,1)\n",
    "    added_image= cv2.resize(added_image, dsize=(1080, 1220))\n",
    "\n",
    "    return added_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "MoviePy error: the file video2_Trim.mp4 could not be found !\nPlease check that you entered the correct path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b6650e505492>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'output4.mp4'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mclip1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"video2_Trim.mp4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mwhite_clip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time white_clip.write_videofile(output, audio=False)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Kiko-PC\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\moviepy\\video\\io\\VideoFileClip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, verbose, fps_source)\u001b[0m\n\u001b[1;32m     79\u001b[0m                                          \u001b[0mtarget_resolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_resolution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                                          \u001b[0mresize_algo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize_algorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                                          fps_source=fps_source)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Make some of the reader's attributes accessible from the clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Kiko-PC\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, print_infos, bufsize, pix_fmt, check_duration, target_resolution, resize_algo, fps_source)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         infos = ffmpeg_parse_infos(filename, print_infos, check_duration,\n\u001b[0;32m---> 32\u001b[0;31m                                    fps_source)\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_fps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Kiko-PC\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py\u001b[0m in \u001b[0;36mffmpeg_parse_infos\u001b[0;34m(filename, print_infos, check_duration, fps_source)\u001b[0m\n\u001b[1;32m    270\u001b[0m         raise IOError((\"MoviePy error: the file %s could not be found !\\n\"\n\u001b[1;32m    271\u001b[0m                       \u001b[0;34m\"Please check that you entered the correct \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                       \"path.\")%filename)\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: MoviePy error: the file video2_Trim.mp4 could not be found !\nPlease check that you entered the correct path."
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "# Reinitialize some global variables.\n",
    "caches = [] \n",
    "\n",
    "output = 'output4.mp4'\n",
    "clip1 = VideoFileClip(\"video2_Trim.mp4\")\n",
    "white_clip = clip1.fl_image(pipeline_final) \n",
    "%time white_clip.write_videofile(output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_image('Untitled.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 608, 608, 3)\n"
     ]
    }
   ],
   "source": [
    "img = np.array([cv2.resize(img, dsize=(608, 608), interpolation=cv2.INTER_CUBIC)])\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t  = model.predict([img, img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_image = reverse_one_hot(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vis_image = colour_code_segmentation(output_image, label_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"%s_pred.png\"%(\"kiko\"),cv2.cvtColor(np.uint8(out_vis_image[0]), cv2.COLOR_RGB2BGR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"%s_pred.png\"%(\"kiko2\"),cv2.cvtColor(np.uint8(img[0]), cv2.COLOR_RGB2BGR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "background = cv2.imread('kiko_pred.png') \n",
    "overlay = cv2.imread('kiko2_pred.png')\n",
    "\n",
    "added_image = cv2.addWeighted(background,1,overlay,1,0)\n",
    "\n",
    "added_image= cv2.resize(added_image, dsize=(608, 608), interpolation=cv2.INTER_CUBIC)\n",
    "cv2.imwrite('combined.png', added_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
